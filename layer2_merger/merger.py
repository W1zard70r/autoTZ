import logging
import networkx as nx
from typing import List
from pydantic import BaseModel, Field
from schemas.graph import ExtractedKnowledge, UnifiedGraph, GraphNode, GraphEdge
from schemas.enums import TZSectionEnum
from utils.llm_client import acall_llm_json

logger = logging.getLogger(__name__)

# –°—Ö–µ–º—ã –¥–ª—è LLM –°–ª–∏—è–Ω–∏—è
class MergeAction(BaseModel):
    is_duplicate: bool = Field(description="–≠—Ç–æ –æ–¥–Ω–∞ –∏ —Ç–∞ –∂–µ —Å—É—â–Ω–æ—Å—Ç—å?")
    ids_to_merge: List[str] = Field(description="–°–ø–∏—Å–æ–∫ ID, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —Å–ª–∏—Ç—å –≤ –æ–¥–∏–Ω")
    unified_id: str = Field(description="–ù–æ–≤—ã–π ID –¥–ª—è —Å–ª–∏—Ç–æ–≥–æ —É–∑–ª–∞")
    unified_name: str = Field(description="–û–±—â–µ–µ –∏–º—è")
    unified_desc: str = Field(description="–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ")

class MergeBatchResult(BaseModel):
    actions: List[MergeAction] = Field(default_factory=list)

class SectionAssignment(BaseModel):
    node_id: str
    target_section: TZSectionEnum

class SectionBatchResult(BaseModel):
    assignments: List[SectionAssignment]

class SmartGraphMerger:
    def __init__(self):
        self.G = nx.DiGraph()

    async def smart_merge(self, subgraphs: List[ExtractedKnowledge]) -> UnifiedGraph:
        logger.info("üîó –°–õ–û–ô 2: –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–¥–≥—Ä–∞—Ñ–æ–≤ –≤ –µ–¥–∏–Ω—ã–π –≥—Ä–∞—Ñ NetworkX")
        
        # 1. –°–∫–ª–∞–¥—ã–≤–∞–µ–º –≤—Å—ë –≤ –æ–¥–∏–Ω –≥—Ä–∞—Ñ
        for sg in subgraphs:
            for node in sg.nodes:
                if not self.G.has_node(node.id):
                    self.G.add_node(node.id, **node.model_dump())
            for edge in sg.edges:
                self.G.add_edge(edge.source, edge.target, **edge.model_dump())

        logger.info(f"  -> –ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {self.G.number_of_nodes()} —É–∑–ª–æ–≤, {self.G.number_of_edges()} —Å–≤—è–∑–µ–π.")

        # 2. –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —É–∑–ª—ã –ø–æ —Ç–∏–ø–∞–º –¥–ª—è –±–∞—Ç—á-—Å–ª–∏—è–Ω–∏—è
        nodes_by_label = {}
        for nid, data in self.G.nodes(data=True):
            label = data.get("label")
            if label not in nodes_by_label:
                nodes_by_label[label] = []
            nodes_by_label[label].append({"id": nid, "name": data.get("name"), "desc": data.get("description")})

        # 3. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ LLM
        for label, nodes in nodes_by_label.items():
            if len(nodes) < 2: continue
            
            logger.info(f"  -> –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –≥—Ä—É–ø–ø—ã '{label}' ({len(nodes)} —É–∑–ª–æ–≤)...")
            # –ë—å–µ–º –Ω–∞ –±–∞—Ç—á–∏ –ø–æ 15 —É–∑–ª–æ–≤, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
            batch_size = 15
            for i in range(0, len(nodes), batch_size):
                batch = nodes[i:i+batch_size]
                await self._resolve_duplicates_batch(batch)

        # 4. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–µ–∫—Ü–∏—è–º –¢–ó
        await self._assign_sections()

        # 5. –≠–∫—Å–ø–æ—Ä—Ç –≤ UnifiedGraph
        final_nodes = [GraphNode(**data) for _, data in self.G.nodes(data=True)]
        final_edges = [GraphEdge(source=u, target=v, **data) for u, v, data in self.G.edges(data=True)]
        
        return UnifiedGraph(nodes=final_nodes, edges=final_edges)

    async def _resolve_duplicates_batch(self, nodes_batch: List[dict]):
        prompt = """–¢—ã –ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä. –ù–∞–π–¥–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã —Å—Ä–µ–¥–∏ —ç—Ç–∏—Ö —É–∑–ª–æ–≤ (—Å–∏–Ω–æ–Ω–∏–º—ã, –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ –ø–æ–Ω—è—Ç–∏–µ).
        –ï—Å–ª–∏ –Ω–∞—Ö–æ–¥–∏—à—å –¥—É–±–ª–∏–∫–∞—Ç—ã, –≤–µ—Ä–Ω–∏ MergeAction —Å is_duplicate=true.
        –ï—Å–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ—Ç, –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ actions."""
        
        data_str = "\n".join([f"ID: {n['id']} | –ò–º—è: {n['name']} | –û–ø–∏—Å–∞–Ω–∏–µ: {n['desc']}" for n in nodes_batch])
        
        try:
            result = await acall_llm_json(schema=MergeBatchResult, prompt=prompt, data=data_str)
            for action in result.actions:
                if action.is_duplicate and len(action.ids_to_merge) > 1:
                    self._merge_nodes_in_graph(action)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {e}")

    def _merge_nodes_in_graph(self, action: MergeAction):
        valid_ids = [nid for nid in action.ids_to_merge if self.G.has_node(nid)]
        if not valid_ids: return

        primary_id = action.unified_id
        if not self.G.has_node(primary_id):
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —É–∑–µ–ª –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–≤–æ–≥–æ —Å—Ç–∞—Ä–æ–≥–æ
            base_data = self.G.nodes[valid_ids[0]].copy()
            base_data.update({
                "id": primary_id,
                "name": action.unified_name,
                "description": action.unified_desc
            })
            self.G.add_node(primary_id, **base_data)

        # –ü–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è–µ–º —Å–≤—è–∑–∏
        for old_id in valid_ids:
            if old_id == primary_id: continue
            
            for u, v, data in list(self.G.edges(old_id, data=True)):
                if u == old_id: self.G.add_edge(primary_id, v, **data)
            
            for u, v, data in list(self.G.in_edges(old_id, data=True)):
                if v == old_id: self.G.add_edge(u, primary_id, **data)
                
            self.G.remove_node(old_id)

    async def _assign_sections(self):
        logger.info("  -> –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–∑–ª–æ–≤ –ø–æ —Å–µ–∫—Ü–∏—è–º –¢–ó...")
        nodes_to_assign = [{"id": n, "name": d.get("name"), "label": d.get("label")} 
                           for n, d in self.G.nodes(data=True) if d.get("label") != "Person"]
        
        if not nodes_to_assign: return

        prompt = """–†–∞—Å–ø—Ä–µ–¥–µ–ª–∏ –∫–∞–∂–¥—ã–π —É–∑–µ–ª –≤ –æ–¥–Ω—É –∏–∑ —Å–µ–∫—Ü–∏–π –¢–ó:
        - GENERAL (–æ–±—â–∞—è –∏–Ω—Ñ–∞, –∑–∞–¥–∞—á–∏)
        - STACK (–∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã, –ë–î, –ª–∏–±—ã)
        - FUNCTIONAL (—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è, —Ñ–∏—á–∏)
        - INTERFACE (–≤—Å—ë –ø—Ä–æ UI/UX)"""

        batch_size = 20
        for i in range(0, len(nodes_to_assign), batch_size):
            batch = nodes_to_assign[i:i+batch_size]
            data_str = "\n".join([f"ID:{n['id']} | {n['label']} | {n['name']}" for n in batch])
            try:
                result = await acall_llm_json(schema=SectionBatchResult, prompt=prompt, data=data_str)
                for assignment in result.assignments:
                    if self.G.has_node(assignment.node_id):
                        self.G.nodes[assignment.node_id]["target_section"] = assignment.target_section
            except Exception:
                pass