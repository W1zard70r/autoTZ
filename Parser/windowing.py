import os
import asyncio
import numpy as np
import networkx as nx
import community as community_louvain
from datetime import datetime, timedelta
from typing import List, Tuple
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

MAX_CHARS_PER_WINDOW = 12000
OVERLAP_MESSAGES = 6
SEMANTIC_THRESHOLD = 0.65
LOOKBACK_WINDOW = 30

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Rate Limiter
EMBEDDING_BATCH_SIZE = 20  # ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ·Ğ° 1 Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
EMBEDDING_DELAY = 10  # Ğ—Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ° Ğ² ÑĞµĞºÑƒĞ½Ğ´Ğ°Ñ… (1.0 ÑĞµĞº = Ğ¼Ğ°ĞºÑ 60 RPM, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ Ğ´Ğ»Ñ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ° 100)


def parse_date(date_str: str) -> datetime:
    try:
        # ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ² Ğ´Ğ°Ñ‚, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚Ğµ, Ñ‡Ñ‚Ğ¾ Ñ 'Z'
        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
    except:
        return datetime.now()


def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    v1 = np.array(vec1)
    v2 = np.array(vec2)
    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:
        return 0.0
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))


async def asplit_chat_into_semantic_threads(messages: List[dict]) -> List[Tuple[str, List[dict], List[dict]]]:
    """
    Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‡Ğ°Ñ‚ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ñ€ĞµĞ´Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ².
    Ğ’ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñƒ Ğ¾Ñ‚ Rate Limit API Google.
    """
    valid_msgs = [m for m in messages if m.get("type") == "message" and m.get("text")]
    if not valid_msgs: return []

    # 1. Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    embeddings_model = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")

    # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² (Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ Ğ¿ÑƒÑÑ‚Ñ‹Ñ… ÑÑ‚Ñ€Ğ¾Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ñ‹Ğ·Ğ²Ğ°Ñ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ API)
    texts_to_embed = [str(m.get("text", "")).strip() or "empty" for m in valid_msgs]
    print(f"ğŸ§  Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ {len(texts_to_embed)} ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ (Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Rate Limit)...")

    # --- RATE LIMITING LOGIC START ---
    embeddings = []
    total_batches = (len(texts_to_embed) + EMBEDDING_BATCH_SIZE - 1) // EMBEDDING_BATCH_SIZE

    for i in range(0, len(texts_to_embed), EMBEDDING_BATCH_SIZE):
        batch = texts_to_embed[i: i + EMBEDDING_BATCH_SIZE]
        current_batch_num = (i // EMBEDDING_BATCH_SIZE) + 1

        try:
            # print(f"   Batch {current_batch_num}/{total_batches}...") # Ğ Ğ°ÑĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ñ Ğ´ĞµĞ±Ğ°Ğ³Ğ°
            batch_result = await embeddings_model.aembed_documents(batch)
            embeddings.extend(batch_result)
        except Exception as e:
            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° Ğ±Ğ°Ñ‚Ñ‡Ğµ {current_batch_num}: {e}")
            # Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ½ÑƒĞ»ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ğ´ĞµĞºÑÑ‹ Ğ¸ Ğ½Ğµ ÑĞ»Ğ¾Ğ¼Ğ°Ñ‚ÑŒ Ğ³Ñ€Ğ°Ñ„
            embeddings.extend([[0.0] * 768] * len(batch))

        # Ğ–Ğ´ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ´ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ñ‹ÑĞ¸Ñ‚ÑŒ 100 RPM
        # 60 RPM = 1 Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ. Ğ­Ñ‚Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾.
        await asyncio.sleep(EMBEDDING_DELAY)
    # --- RATE LIMITING LOGIC END ---

    # 2. Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ğ¼ ÑƒĞ·Ğ»Ñ‹ Ğ³Ñ€Ğ°Ñ„Ğ°
    G = nx.Graph()
    for i, msg in enumerate(valid_msgs):
        G.add_node(msg["id"], msg=msg, vec=embeddings[i], time=parse_date(msg.get("date", "")))

    # 3. Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ ÑĞ²ÑĞ·Ğ¸ (Ğ¯Ğ²Ğ½Ñ‹Ğµ + Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ)
    print("ğŸ”— ĞŸĞ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ ÑĞ²ÑĞ·ĞµĞ¹...")
    for i, msg in enumerate(valid_msgs):
        reply_id = msg.get("reply_to_message_id")

        # Ğ. Ğ¯Ğ²Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ (Reply)
        if reply_id and G.has_node(reply_id):
            G.add_edge(msg["id"], reply_id, type="reply")
            continue

        # Ğ‘. ĞĞµÑĞ²Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ (Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ°)
        best_sim = 0.0
        best_target_id = None
        start_idx = max(0, i - LOOKBACK_WINDOW)

        for j in range(start_idx, i):
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ (Ğ½Ğµ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ĞµĞ¼ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†ĞµĞ¹ > 4 Ñ‡Ğ°ÑĞ¾Ğ²)
            time_diff = G.nodes[msg["id"]]["time"] - G.nodes[valid_msgs[j]["id"]]["time"]
            if time_diff > timedelta(hours=4):
                continue

            sim = cosine_similarity(embeddings[i], embeddings[j])
            if sim > best_sim:
                best_sim = sim
                best_target_id = valid_msgs[j]["id"]

        if best_sim >= SEMANTIC_THRESHOLD and best_target_id:
            G.add_edge(msg["id"], best_target_id, type="semantic", weight=best_sim)

    # 4. Community Detection (Louvain)
    # Ğ’ĞĞ–ĞĞ: Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ĞŸĞĞ¡Ğ›Ğ• Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ±ĞµÑ€, Ğ¸Ğ½Ğ°Ñ‡Ğµ Ğ³Ñ€Ğ°Ñ„ Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹
    if G.number_of_edges() > 0:
        try:
            print("ğŸ” Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Community Detection (Louvain)...")
            partition = community_louvain.best_partition(G)  # Louvain Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ½ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ°Ğ¼Ğ¸
            for node_id, comm_id in partition.items():
                G.nodes[node_id]["community"] = comm_id
        except Exception as e:
            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Louvain: {e}. ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ÑÑ‚Ğ°Ğ¿ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ².")

    # 5. Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ñ‚Ñ€ĞµĞ´Ñ‹ (Ğ¡Ğ²ÑĞ·Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹)
    threads = []
    for component in nx.connected_components(G):
        thread_msgs = [G.nodes[node_id]["msg"] for node_id in component]
        thread_msgs.sort(key=lambda x: parse_date(x.get("date", "")))
        threads.append(thread_msgs)

    threads.sort(key=lambda t: parse_date(t[0].get("date", "")))

    # 6. ĞĞ°Ñ€ĞµĞ·ĞºĞ° Ğ½Ğ° Ğ¾ĞºĞ½Ğ°
    processed_windows = []
    for thread_idx, thread in enumerate(threads):
        current_window = []
        current_chars = 0

        for msg in thread:
            msg_len = len(str(msg.get("text", "")))

            if current_chars + msg_len > MAX_CHARS_PER_WINDOW and len(current_window) > OVERLAP_MESSAGES:
                start_id = current_window[0]["id"]
                end_id = current_window[-1]["id"]
                window_ref = f"thread_{thread_idx}_msg_{start_id}_to_{end_id}"

                processed_windows.append((window_ref, current_window, []))

                current_window = current_window[-OVERLAP_MESSAGES:]
                current_chars = sum(len(str(m.get("text", ""))) for m in current_window)

            current_window.append(msg)
            current_chars += msg_len

        if current_window:
            start_id = current_window[0]["id"]
            end_id = current_window[-1]["id"]
            window_ref = f"thread_{thread_idx}_msg_{start_id}_to_{end_id}"
            processed_windows.append((window_ref, current_window, []))

    print(f"âœ… Ğ§Ğ°Ñ‚ Ñ€Ğ°Ğ·Ğ±Ğ¸Ñ‚ Ğ½Ğ° {len(processed_windows)} ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ĞºĞ¾Ğ½.")
    return processed_windows


def split_text_into_chunks(text: str) -> List[Tuple[str, str]]:
    splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=500)
    chunks = splitter.split_text(text)
    return [(f"chunk_{i + 1}", chunk) for i, chunk in enumerate(chunks)]